{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4891f21c",
   "metadata": {},
   "source": [
    "# UE 4 Mehrklassenklassifikation mit Deep Learning Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94234d2e",
   "metadata": {},
   "source": [
    "In der dritten Übungsaufgabe habt ihr eine Klassifikationspipeline für 15 Klassen aus dem **Subset InVar-100** Datensatze aufgebaut und eure Ergebnisse evaluiert. \n",
    "\n",
    "\n",
    "\n",
    "## Nachtrag UE 3\n",
    "Lösungsansätze für HOG-SVM-Klassifikation und HOG-Multi-layer-Perceptron-Klassifikation\n",
    "### HOG-SVM-Klassifikation\n",
    "\n",
    "In dieser Aufgabe solltet ihr eure eure Klassifikationspipeliene aus der UE 3 auf 15 Klassen des Industrial 100 Datensatzes erweitern und die Evaluation der Ergebnisse durchführen.\n",
    "\n",
    "Für diese Aufgabe wurde sklearn.[svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)-Klassifikator verwendet. \n",
    "\n",
    "Dieser Klassifikator bietet folgende Vorteile: \n",
    "- Wirksam in höherdimensionalen Merkmalsräumen\n",
    "- Wirksam, sogar wenn die Anzahl der Dimensionen größer ist als die Anzahl der Samples \n",
    "\n",
    "Die im Folgenden abgebildete Confusion Matrix visualisiert beispielhaft die Ergebnisse für diese Aufgabe.\n",
    "Übersicht der Ergebnisse:\n",
    "- Number of mislabeled points out of a total 195 points : 11\n",
    "- Accuracy score for HOG-svm.svc is: 0.94\n",
    "***\n",
    "\n",
    "**Confusion Matrix** für HOG-SVM-Klassifikator\n",
    "<img src=\"./ipynb_bilder/cm_svm_svc_acc_94_all_classes.png\"  />\n",
    "\n",
    "Zum Extrahieren der HOG-Features wurde folgende Konfiguration verwendet: \n",
    "\n",
    "- img = imread(img_path)\n",
    "- img = resize(img, (128, 128),anti_aliasing=True)\n",
    "- hog_fd = hog(img, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), channel_axis=-1)\n",
    "***\n",
    "\n",
    "*train_test_split_fct* - Parameter:\n",
    "\n",
    "- test_size=0.25\n",
    "- random_state=42\n",
    "***\n",
    "\n",
    "Code für svm.SVC-Klassifikation:\n",
    "\n",
    "- from sklearn import svm\n",
    "- clf = svm.SVC()\n",
    "- clf.fit(X_train_hog, y_train_hog)\n",
    "- print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test_hog.shape[0], \n",
    "                                                                    (y_test_hog != clf.predict(X_test_hog)).sum()))\n",
    "\n",
    "- accuracy_score_hog_svc = accuracy_score(y_test_hog, clf.predict(X_test_hog))\n",
    "- print('Accuracy score for HOG-svm.svc is: {}'.format(accuracy_score_hog_svc))\n",
    "\n",
    "### HOG-Multi-layer-Perceptron-Klassifikation\n",
    "\n",
    "Anbei ist die Codevorlage und beispielhahte Visualisierung der Ergebnisse:\n",
    "\n",
    "- Number of mislabeled points out of a total 195 points : 14\n",
    "- Accuracy score for HOG-MLP is: 0.928\n",
    "***\n",
    "**Confusion Matrix** für HOG-MLP-Klassifikator\n",
    "<img src=\"./ipynb_bilder/cm_hog_mlp_acc_93_all_classes.png\"  />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f830cdc1",
   "metadata": {},
   "source": [
    "## Vorbereitende Aufgabe UE 4 - Theorie\n",
    "\n",
    "In der heutigen Übung macht ihr euch die Deep Learning Frameworks zunutze, um die Klassifikation ohne explizite Merkmalsextraktion durchzuführen.\n",
    "\n",
    "\n",
    "In der Übung lernt ihr [Tensorflow-Open-Source-Plattform](https://www.tensorflow.org) und [Keras](https://keras.io/) kennen. \n",
    "\n",
    "\n",
    "\n",
    "Die **Tensorflow-Tutorials für Anfänger** https://www.tensorflow.org/tutorials?hl=de könnt ihr als Einstieg in die Keras-Grundlagen nutzen.\n",
    "\n",
    "Das Ziel ist, dass ihr euch mit den **4 Schritten einer Modellentwicklung mit Keras** vertraut macht:\n",
    "\n",
    "1. Aufbau eines Modells\n",
    "2. Kompilieren eines Modells\n",
    "3. Training eines Modells \n",
    "4. Evaluation des trainierten Modells\n",
    "\n",
    "\n",
    "Folgende Fragen solltet ihr dabei beantwortet haben: \n",
    "- Wie erfolgt die Datenaufbereitung („Loading & Preprocessing“)?\n",
    "- Was ist ein Layer? \n",
    "- Welche Modelltypen bietet Keras an?\n",
    "- Was ist ein Sequential model? \n",
    "- Wofür werden folgende Befehle / Funktionen benutzt, welche Parameter müssen für die Verwendung dieser Befehle in Abhängigkeit von der Aufgabenstellung und den verwendeten Daten festgelegt werden: \n",
    "    - Model()\n",
    "    - Sequential()\n",
    "    - keras.layers.Dense()\n",
    "    - add()\n",
    "    - compile()\n",
    "    - fit()\n",
    "    - evaluate()\n",
    "    - predict()\n",
    "    \n",
    "Als Informationsquelle bietet sich auch https://keras.io/ an. Gerne könnt ihr euch alles anschauen, fokussiert euch in diesem Schritt primär auf die beiden **Beispiele** mit den Hinweisen zu den einzelnen Schritten: \n",
    "- https://www.tensorflow.org/tutorials/images/classification\n",
    "- https://keras.io/getting_started/intro_to_keras_for_engineers/ \n",
    "\n",
    "\n",
    "Ich werde diese Aufgabe mit euch in der Übung gemeinsam duchgehen und die einzelnen Schritte erläutern. \n",
    "\n",
    "Ihr könnt euch aber auch selbstständig den ersten Überblick verschaffen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225ba3df",
   "metadata": {},
   "source": [
    "## Praktische Aufgabe \n",
    "### Klassifikation mit einem ConvNet-Modell\n",
    "\n",
    "In dieser Aufgabe werdet ihr eine andere Netzarchitektur – Convolutional Neural Networks (ConvNets) – kennenlernen. Diese Netzarchitektur wurde speziell für die Verarbeitung von Bildern konzipiert. Zusätzlich zu den Dense-Layers, die für die Klassifikationsaufgabe zuständig sind, kommen zwei weitere ConvNets Layer-Typen, die die Merkmalsextraktionsaufgabe übernehmen. Das sind Convolutional Layers und Pooling Layers.\n",
    "\n",
    "\n",
    "Eure Aufgabe besteht darin, ein simples ConvNet-Klassifikationsmodell zu implementieren, zu trainieren und zu testen. \n",
    "\n",
    "Ihr werdet einen **Subset aus dem Industrial 100** Datensatz verwenden. Der Subset enthält die ersten 15 Klassen des Datensatzes. Eine Übersicht über alle ClassIDs und deren Bezeichnungen findet ihr in der csv-Datei Industrial100-labels.csv.\n",
    "Den Datensatz könnt ihr (wenn nicht in der dritten Übung gemacht) von der tubCloud herunterladen und in den BGA2-Ordner ablegen (oder auf eurem eigenen Rechner). Anbei der Link zum Datensatz: https://tubcloud.tu-berlin.de/s/7ZRADfF4kdJGSma\n",
    "\n",
    "Diese Aufgabe könnt ihr angelehnt an die Schritte der Modellentwicklung mit Keras in folgende Teilaufgaben unterteilen:\n",
    "1. Datenaufbereitung\n",
    "2. Aufbau des Modells\n",
    "3. Kompilieren des Modells\n",
    "4. Training des Modells\n",
    "5. Evaluation des trainierten Modells\n",
    "6. Speichern des Modells\n",
    "7. Verwendung des gespeicherten Modells zur Klassifikation eigener Beispielbilder\n",
    "\n",
    "Zur optimalen Nutzung des Arbeitsspeichers deines Rechners könnt ihr zum Laden von Daten die Funktion [image_dataset_from_directory](https://keras.io/api/preprocessing/image/#image_dataset_from_directory-function)  nutzen. \n",
    "\n",
    "Alternative [Moeglichkeiten](https://keras.io/api/preprocessing/image/): \n",
    "\n",
    "- ImageDataGenerator + flow_from_directory()\n",
    "- ImageDataGenerator + flow_from_dataframe()\n",
    "- load_img()\n",
    "- img_to_array()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee38b1b",
   "metadata": {},
   "source": [
    "### Zusätzuliche Ressourcen\n",
    "Schaut euch die im Übungsordner abgelegte Dokumentation mit den ersten Hinweisen zu Google [Colab](https://colab.research.google.com/notebooks/intro.ipynb) und den Introlink. \n",
    "\n",
    "Ihr könnt diese Umgebung für die Übung 4 und 5 nutzen. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249d526f-27b7-4f06-b28a-cfc074729e11",
   "metadata": {},
   "source": [
    "### Ab hier - Eure Implementierung anhand der Beispiele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14352eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import layers, models, regularizers\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66355439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 780 files belonging to 15 classes.\n",
      "Using 624 files for training.\n",
      "Found 780 files belonging to 15 classes.\n",
      "Using 156 files for validation.\n",
      "['0', '1', '10', '11', '12', '13', '14', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "# 1. Datenaufbereitung\n",
    "data_dir = \"/Users/chexuanyou/TUB/SS_25/ABGA2/ABGA2/UE_4_Aufgaben-20250618/Industrial100_Auszug/SUBSET_Industrial_100\"\n",
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "seed = 42\n",
    "\n",
    "train_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=seed,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=seed,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e9284e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, regularizers\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(224, 224, 3)),\n",
    "    layers.Rescaling(1./255),\n",
    "\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(15, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c5cda543-e9c9-449f-ac1d-6eebd02a5ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Kompilieren des Modells\n",
    "model.compile(\n",
    "    optimizer='adam',  # 优化器\n",
    "    loss='sparse_categorical_crossentropy',  # 多类分类的损失函数\n",
    "    metrics=['accuracy']  # 评估指标\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "67c8c626-8445-4c4c-be0d-13e7ca91a9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 734ms/step - accuracy: 0.0850 - loss: 3.8473 - val_accuracy: 0.2179 - val_loss: 2.6714\n",
      "Epoch 2/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 681ms/step - accuracy: 0.1491 - loss: 2.5861 - val_accuracy: 0.3910 - val_loss: 2.1035\n",
      "Epoch 3/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 697ms/step - accuracy: 0.3361 - loss: 2.0616 - val_accuracy: 0.4359 - val_loss: 1.8332\n",
      "Epoch 4/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 721ms/step - accuracy: 0.4549 - loss: 1.6598 - val_accuracy: 0.5577 - val_loss: 1.4347\n",
      "Epoch 5/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 752ms/step - accuracy: 0.6191 - loss: 1.1990 - val_accuracy: 0.6474 - val_loss: 1.2399\n",
      "Epoch 6/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 753ms/step - accuracy: 0.6493 - loss: 1.0317 - val_accuracy: 0.6538 - val_loss: 1.0382\n",
      "Epoch 7/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 880ms/step - accuracy: 0.7382 - loss: 0.7772 - val_accuracy: 0.6923 - val_loss: 0.9838\n",
      "Epoch 8/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 840ms/step - accuracy: 0.7666 - loss: 0.6620 - val_accuracy: 0.7244 - val_loss: 0.9988\n",
      "Epoch 9/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 840ms/step - accuracy: 0.7565 - loss: 0.6922 - val_accuracy: 0.7115 - val_loss: 1.2327\n",
      "Epoch 10/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 830ms/step - accuracy: 0.8067 - loss: 0.6393 - val_accuracy: 0.7436 - val_loss: 0.9264\n",
      "Epoch 11/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 806ms/step - accuracy: 0.8282 - loss: 0.5106 - val_accuracy: 0.7308 - val_loss: 0.9390\n",
      "Epoch 12/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 813ms/step - accuracy: 0.8592 - loss: 0.4594 - val_accuracy: 0.7500 - val_loss: 0.8244\n",
      "Epoch 13/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 802ms/step - accuracy: 0.7718 - loss: 0.5915 - val_accuracy: 0.7179 - val_loss: 0.9618\n",
      "Epoch 14/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 812ms/step - accuracy: 0.8449 - loss: 0.4748 - val_accuracy: 0.7308 - val_loss: 1.0397\n",
      "Epoch 15/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 798ms/step - accuracy: 0.8872 - loss: 0.3647 - val_accuracy: 0.7179 - val_loss: 1.2726\n"
     ]
    }
   ],
   "source": [
    "# 定义 EarlyStopping 回调\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# 训练模型\n",
    "epochs = 15\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "44da16a3-ba5c-4edf-8d2c-4e8c4c7d41d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 220ms/step - accuracy: 0.7339 - loss: 0.8694\n",
      "Validation accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "# 5. Evaluation des trainierten Modells\n",
    "loss, acc = model.evaluate(val_ds)\n",
    "print(f\"Validation accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "338b6d53-2fa9-447f-a52d-ee516a34fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Speichern des Modells\n",
    "model.save(\"my_convnet_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d4bda7b-2804-4053-a511-034d31b5b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Verwendung des gespeicherten Modells zur Klassifikation eigener Beispielbilder\n",
    "\n",
    "model = load_model(\"my_convnet_model.keras\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3e3a39c6-bdd5-47cc-b16b-7fe1ebb133f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
      "Predicted class ID: 13\n",
      "Predicted label name: Edelstahlschraube-DIN912\n"
     ]
    }
   ],
   "source": [
    "# 加载图片并预处理（和训练保持一致）\n",
    "img_path = \"/Users/chexuanyou/TUB/SS_25/ABGA2/ABGA2/UE_4_Aufgaben-20250618/Industrial100_Auszug/SUBSET_Industrial_100/8/Industrial_9.jpg\"\n",
    "img = image.load_img(img_path, target_size=(224, 224))  # Resize\n",
    "img_array = image.img_to_array(img)             # Normalize\n",
    "img_array = np.expand_dims(img_array, axis=0)           # Add batch dim\n",
    "\n",
    "# 预测\n",
    "pred = model.predict(img_array)\n",
    "predicted_class = np.argmax(pred, axis=1)[0]\n",
    "print(f\"Predicted class ID: {predicted_class}\")\n",
    "\n",
    "\n",
    "label_map = pd.read_csv(\n",
    "    \"/Users/chexuanyou/TUB/SS_25/ABGA2/ABGA2/UE_4_Aufgaben-20250618/Industrial100_Auszug/SUBSET_Industrial_100/Industrial100-labels.csv\",\n",
    "    header=None, names=[\"class_id\", \"label\"]\n",
    ")\n",
    "\n",
    "print(\"Predicted label name:\", label_map.iloc[predicted_class]['label'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
